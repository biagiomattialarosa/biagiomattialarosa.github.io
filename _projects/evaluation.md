---
layout: page
title: Evaluating Explanations
description: The goal of this research area is to understand the motivations behind specific predictions of a deep neural network. understanding what deep neural networks learn during the training process. Recently this area has been moved under the umbrella name of "Mechanistic Interpretability"
img: assets/img/plots.png
importance: 3
category: available
related_publications: true
---
This page provides a summary sheet that includes the general goal, reference papers (both mine and external) for an overview of the topic, as well as the domains explored so far. We are also interested in extending the applications of these techniques beyond their traditional domains. If you have expertise in other areas (e.g., neuroscience, gaming, or audio/speech modeling), we would be happy to explore potential extensions into those fields.

**Goal:** The goal of this research area is to standardize the evaluation procedures for explanations. This research directions aims to create novel metrics to measure properties of interest, unify the existing ones, discover pitfall on the current evaluation procedures, and develop novel benchmarks. 

**Domains:** Vision.

**Reference Papers**: 
1. Metrics for Neuron Explanations: [{% cite LaRosa2023Towards %}] [{% cite Makinwa2022 %}] 
2. Metrics for Memory-based Explanations: [{% cite LaRosa2022 %}]

