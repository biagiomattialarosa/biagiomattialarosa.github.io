---
layout: page
title: Interpretable Deep Neural Networks
description: This research direction investigates designs of self-interpretable deep neural networks—AI systems capable of providing explanations alongside their predictions. This involves either generalizing existing designs to novel domains and tasks or creating entirely new designs from scratch. 
img: assets/img/memory.png
importance: 1
category: available
related_publications: true
---
This page provides a summary sheet that includes the general goal, reference papers (both my own and external) for an overview of the topic, and the domains explored. We are particularly interested in extending the application of these techniques beyond their traditional domains. If you have expertise in other areas (e.g., neuroscience, games, or audio models), we would be excited to explore potential extensions into those fields.

**Goal**:  The goal of this research direction is to develop self-interpretable deep neural networks—AI systems capable of providing explanations alongside their predictions. This involves either generalizing existing designs to novel domains and tasks or creating entirely new designs from scratch. Thus far, our work has focused on leveraging memory mechanisms, prototypes, and interpretable axes as foundational elements for these designs.

**Domains**: NLP, Vision, Chemistry, Robot Navigation,

**Reference Papers**: 
1. Memory: [{% cite LaRosa2022 %}]
2. Protoypes-based [{% cite Ragno2022 %}] [<a href="https://arxiv.org/abs/1806.10574">Seminal Paper</a>] 
3. Interpretable Axes [{% cite Proietti2023 %}] [<a href="https://arxiv.org/abs/2002.01650">Seminal Paper</a>] 