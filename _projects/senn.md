---
layout: page
title: Self-Interpretable Deep Neural Networks
description: This research direction investigates designs of self-interpretable deep neural networks capable of providing explanations alongside their predictions. This involves either generalizing existing designs to novel domains and tasks or creating new ones from scratch. 
img: assets/img/memory.png
importance: 1
category: available
related_publications: true
---
This page provides a summary sheet that includes the general goal, reference papers (both mine and external) for an overview of the topic, as well as the domains explored so far. We are also interested in extending the applications of these techniques beyond their traditional domains. If you have expertise in other areas (e.g., neuroscience, gaming, or audio/speech modeling), we would be happy to explore potential extensions into those fields.

**Goal**:  The goal of this research direction is to develop self-interpretable deep neural networks capable of providing explanations alongside their predictions. This involves either generalizing existing designs to novel domains and tasks or creating new ones from scratch. Thus far, our work has focused on leveraging memory mechanisms, prototypes, and interpretable axes as foundational elements for these designs.

**Domains**: NLP, Vision, Chemistry, Robot Navigation.

**Reference Papers**: 
1. Memory: [{% cite LaRosa2022 %}]
2. Protoypes-based [{% cite Ragno2022 %}] [<a href="https://arxiv.org/abs/1806.10574">Seminal Paper</a>] 
3. Interpretable Axes [{% cite Proietti2023 %}] [<a href="https://arxiv.org/abs/2002.01650">Seminal Paper</a>] 