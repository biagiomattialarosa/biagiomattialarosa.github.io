---
layout: page
title: Explaining Neurons
description: This research area aims to understand what deep neural networks learn during the training process. Specifically, we are interested on analyzing the behavior of individual neurons and groups of neurons and discovering the concepts they learn to recognize and the relationships between these concepts. 
img: assets/img/neurons.jpg
importance: 2
category: available
related_publications: true
---
This page provides a summary sheet that includes the general goal, reference papers (both mine and external) for an overview of the topic, as well as the domains explored so far. We are also interested in extending the applications of these techniques beyond their traditional domains. If you have expertise in other areas (e.g., neuroscience, gaming, or audio/speech modeling), we would be happy to explore potential extensions into those fields.

**Goal**: The goal of this research area is to understand what deep neural networks learn during the training process. Recently, this field has been categorized under the umbrella term `Mechanistic Interpretability`. My reseach focuses on analyzing the behavior of individual neurons and groups of neurons and discovering the concepts they learn to recognize and the relationships between these concepts. My projects typically combine tools from classical AI (e.g., heuristic search and clustering), statistical analysis, and recent advancements in AI to explore this direction.

**Domains**: NLP, Vision. 

**Reference Papers**: 
1. Logical (Compositional) Explanations: [{% cite LaRosa2023Towards %}] [{% cite Makinwa2022 %}] 
2. Linear Explanations: [<a href="https://arxiv.org/abs/2405.06855">Link</a>]
3. Circuits (chain of neurons): [<a href="https://distill.pub/2020/circuits/zoom-in/">Link</a>] 

